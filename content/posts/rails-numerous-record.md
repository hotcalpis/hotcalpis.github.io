---
title: '大量レコードを自力で処理する際のやらかしポイント'
date: '2024-03-02'
description: ''
---

仕事で売上レコードのCSVエクスポート機能を実装したのですが、予想以上に落とし穴が多く、インシデントを起こしてしまいました。
反省として大量レコードを扱う際の注意点をまとめます。

なお、Rails を前提としていますが、他のフレームワークにも通じるはずです。

## 落とし穴

### 0. 一括で処理するとメモリが不足する

常識レベルですが、万単位の大量レコードを全件一括でループ処理してはいけません。
ActiveRecord インスタンスはメモリを喰うため、 **1000件程度に分割して処理** しないとサーバー障害を引き起こしてしまう危険があります。

そこで Rails の場合、`in_batches` `find_in_batches` `find_each` が用意されているので基本的にはこれらを使うようにしましょう。
ただし、以下の欠点があるので場合によっては自力で分割処理を実装する必要があります。

- order が指定できない
- Layered Architecture などでは採用しづらい

SQL の OFFSET で実装して一件落着…としたいところですが、実はそう簡単ではありません。
本題ですが、分割処理を OFFSET などで安直に実装すると不具合が起こります。
これから一つずつ解説していきます。

### 1. SQL キャッシュによりメモリが浪費される

こちらは単純な話ですが見落としがちです。
Rails ではデフォルトで [SQLキャッシュ](https://railsguides.jp/caching_with_rails.html#sql%E3%82%AD%E3%83%A3%E3%83%83%E3%82%B7%E3%83%A5) という機能が効いており、**SQL の実行結果が自動的にキャッシュされるようになっています。**

これが悪影響し、キャッシュする必要のないレコード情報がメモリ上にどんどんキャッシュされていってしまうのです。

ActiveRecord の実装では [このあたり](https://github.com/rails/rails/blob/5d528ba0c82f77342d62f4872b409f87890d05b1/activerecord/lib/active_record/relation/batches.rb#L368) で考慮がされているようなので、`in_bathces` などを使っていれば問題ありませんが、自力で実装する際はこれと同じ考慮をする必要があるわけです。

回避策は、`ActiveRecord::Base.uncached` を使うといいと思います。
ただし、キャッシュしたい SQL まで一緒に無効化してしまわないように気をつけましょう。

### 2. OFFSET が大きくなるにつれて SQL が遅くなる

こちらは OFFSET の動作原理によって発生する問題です。
OFFSET を指定した場合、ORDER BY の順でレコードを読み出し、検索条件に合うものをカウントしていき、指定した範囲のレコードを得るような挙動になります。
これにより **OFFSET が後ろの方に進んでいくにつれて SQL が遅くなっていきます。**

回避策として、シーク法か、IDだけ先に全件取得しておいて1000件ずつ引き直す方法があります。

### 3. トランザクションの有無や隔離性水準によってファントムリードが起きる

こちらは表題そのままです。
同じ検索条件で複数回クエリを実行することになるので、ファントムリードを考慮する必要があります。
例えば、`id: 1..1000` のレコードを取得した後で別トランザクションにより `id: 1` のレコードが削除された場合、2回目の SQL は `id: 1002..2001` を検索することになり、結果的に `id: 1001` のレコードが欠落してしまいます。

回避策は同上および、トランザクションを見直すことです。

### 4. ORDER が一意でない場合、レコードが重複・欠落する

ORDER BY が一意でない場合、1回目と2回目の SQL でレコードの並び順が変わることがあります。
例えば、`ORDER BY created_at` で同じ created_at のレコードが複数あり、それが `id: 990..1010` のように分割の区切りを跨いでいる場合、1回目の SQL では `990, 991, ...` の並びで `id: 990..1000` がヒットするのに対し、2回目の SQL では `id: 1010, 1009, ...` の並びで再び `id: 1000..990` がヒットしてしまう可能性があります。

回避策は同上に加え、ORDER BY が一意になるように第二ソートキーを指定する方法があります。

### 5. 別トランザクションにより消されたレコードを参照しようとしてエラー

IDだけ先に取得する方法での落とし穴です。
以下のような実装をすると、エラーとなってしまいます。
自分はこれでインシデントを起こしてしまいました。

```ruby
purchase_ids = PurchaseRepository.list_targets # この時点で売上が存在

purchase_ids.each_slice(1000) do |ids|
  purchases_hash = PurcahseRepository.hash_by_ids(ids) # この時点で別トランザクションにより削除済み

  ids.each do |id|
    purcahse = purchase_hash[id] # nil
    do_something if purchase.is_hoge # エラー
  end
end
```

普段の実装では処理対象を一括で読み込み処理するため、このような考慮は基本的に不要です。
それゆえ、気づかずに実装してしまいました…

正直、防ぎにくいバグだと思います。
実装時の注意とコードレビューで気づくしかないかもしれません…

### 番外編

#### tmpfs

CSVエクスポート機能では最終的なファイルサイズが100MB以上になったりするので、ファイルの中身をメモリ上に展開したくありません。
そこで一時ファイルを作成して1000件ずつなどで書き込んでいくことになりますが、そのファイルを tmpfs においてしまうと結局メモリを消費してしまいます。
基本的に意識しないでいいと思いますが、そういったケースもあると知っておくと良いと思います。

#### 24時間稼働の場合は metadata lock を嫌って長いトランザクションを避ける

3番や5番の落とし穴を避けるためにはトランザクションを適切に張ることが重要です。
ただし、24時間無停止のサービスが要件として求められる場合、大量エクスポートのような metadata lock の長いトランザクションがあるとリリース時のマイグレーションの邪魔になってしまったりします。
プロジェクトの特性を考慮してトランザクションを張るべきかどうか決められると更に良いでしょう。

## まとめ

こんなに落とし穴が多く難しいとは思っていませんでした。
ただ、非常に勉強になったので今後もこのような観点を増やして実装に活かしていきたいです。
